{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71afa6b7",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9cb6ed",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import tqdm.notebook as tqdm\n",
    "from torch import optim\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from collections import Counter\n",
    "from collections import OrderedDict, defaultdict\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cbab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 28 13:58:15 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla M40 24GB      Off  | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P8    16W / 250W |      3MiB / 22945MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla M40 24GB      Off  | 00000000:82:00.0 Off |                    0 |\r\n",
      "| N/A   41C    P8    18W / 250W |      3MiB / 22945MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7b3135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac72555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir =  \"../cache\"\n",
    "data_dir = \"../data/mimic3_new\"\n",
    "ID_COL_NAME = \"HADM_ID\"\n",
    "LABEL_COL_NAME = \"ICD9_CODE\"\n",
    "TEXT_COL_NAME = \"TEXT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69129c7b",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7afed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "n_epoch=10\n",
    "optimiser=\"adamw\" # \"adam\", \"sgd\", \"adadelta\", \"adamw\",\"adagrad\"\n",
    "main_metric=\"micro_f1\"\n",
    "multilabel=1\n",
    "shuffle_data=1\n",
    "dropout=0.3\n",
    "max_seq_length=2500\n",
    "min_seq_length=-1\n",
    "min_word_frequency=-1\n",
    "embedding_mode=\"word2vec\"\n",
    "embedding_size=100\n",
    "embedding_file=\"../data/mimic3_new/processed_50.embed\"\n",
    "d_a=256 #\"The dimension of the first dense layer for self attention\"\n",
    "n_labels=50\n",
    "\n",
    "#RNN\n",
    "hidden_size =  256\n",
    "n_layers = 1\n",
    "bidirectional =1\n",
    "use_last_hidden_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3ef67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self,\n",
    "                 training_data: list,\n",
    "                 training_labels: list,\n",
    "                 min_word_frequency: int = -1,\n",
    "                 max_vocab_size: int = -1,\n",
    "                 word_embedding_mode: str = \"word2vec\",\n",
    "                 word_embedding_file: str = None,\n",
    "                 use_gpu: bool = True\n",
    "                 ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.word_embedding_mode = word_embedding_mode\n",
    "        self.word_embedding_file = word_embedding_file\n",
    "        self.word_embedding_size = 100\n",
    "        self.word_embeddings = None\n",
    "\n",
    "        self.training_data = training_data\n",
    "\n",
    "        self.PAD_TOKEN = '_PAD'\n",
    "        self.UNK_TOKEN = '_UNK'\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "\n",
    "        self.label2index = None\n",
    "        self.index2label = None\n",
    "\n",
    "        self.vocab_words = [self.PAD_TOKEN, self.UNK_TOKEN]\n",
    "        self.all_labels = []\n",
    "\n",
    "        self.min_word_frequency = min_word_frequency\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "        self.update_labels(training_labels)\n",
    "\n",
    "    def index_of_word(self,word: str) -> int:\n",
    "        try:\n",
    "            return self.word2index[word]\n",
    "        except:\n",
    "            return self.word2index[self.UNK_TOKEN]\n",
    "\n",
    "    def index_of_label(self,label: str) -> int:\n",
    "        try:\n",
    "            return self.label2index[label]\n",
    "        except:\n",
    "            return 0\n",
    "    def update_labels(self, labels):\n",
    "        self.all_labels = []\n",
    "        self.index2label = []\n",
    "        self.label2index = []\n",
    "        all_labels = list(sorted(labels))\n",
    "        self.label2index = {label: idx for idx, label in enumerate(all_labels)}\n",
    "        self.index2label = {idx: label for idx, label in enumerate(all_labels)}\n",
    "        self.all_labels = all_labels\n",
    "            \n",
    "    def prepare_vocab(self):\n",
    "        self._build_vocab()\n",
    "\n",
    "        # load pretrain word embeddings\n",
    "        if self.word_embedding_file is not None:\n",
    "            self.word_embeddings = torch.FloatTensor(self._load_embeddings())\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        all_words_df = pd.read_csv('../data/mimic3_new/vocab.csv',header=None)\n",
    "        all_words = all_words_df.iloc[:,0].tolist()\n",
    "        all_words.sort()\n",
    "\n",
    "        self.vocab_words += all_words\n",
    "\n",
    "        self.word2index = {word: idx for idx, word in enumerate(self.vocab_words)}\n",
    "        self.index2word = {idx: word for idx, word in enumerate(self.vocab_words)}\n",
    "\n",
    "\n",
    "    def _load_embeddings(self):\n",
    "        if self.word_embedding_file is None:\n",
    "            return None\n",
    "        return self._load_word_embeddings()\n",
    "\n",
    "    def _load_word_embeddings(self):\n",
    "        unknown_vec = np.random.uniform(-0.25, 0.25, self.word_embedding_size)\n",
    "        embeddings = [unknown_vec] * (len(self.vocab_words))\n",
    "        embeddings[0] = np.zeros(self.word_embedding_size)\n",
    "        for line in open(self.word_embedding_file, \"rt\"):\n",
    "            split = line.rstrip().split(\" \")\n",
    "            word = split[0]\n",
    "            vector = np.array([float(num) for num in split[1:]]).astype(np.float32)\n",
    "#             print(word,': ',vector)\n",
    "            if len(vector) > 0:\n",
    "                if word in self.word2index:\n",
    "                    embeddings[self.word2index[word]] = vector\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "        return embeddings\n",
    "    def n_words(self):\n",
    "        return len(self.vocab_words)\n",
    "\n",
    "    def n_labels(self):\n",
    "        return len(self.all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5dfc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path) -> list:\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    id_data = data[ID_COL_NAME]\n",
    "    text_data = data[TEXT_COL_NAME]\n",
    "\n",
    "    hierarchical_label_data = []\n",
    "    label_data = data[LABEL_COL_NAME].tolist()\n",
    "    \n",
    "    output = []\n",
    "    for i in tqdm.tqdm(range(len(label_data)), desc=\"Reading data\"):\n",
    "        labels = label_data[i].split(';')\n",
    "        print(labels)\n",
    "        output.append((text_data[i], labels, id_data[i]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a07b35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_data(file_path: str) -> tuple:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        f.close()\n",
    "    return data[\"vocab\"], data[\"training_data\"], data[\"valid_data\"], data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6718ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    cache_folder = \"{}/{}\".format(cache_dir, \"mimic3_single_50\")\n",
    "#     device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    save_vocab_file_name = \"{}.pkl\".format(\"save_vocab\")\n",
    "    cached_file_name = os.path.join(cache_folder, save_vocab_file_name)\n",
    "    if os.path.exists(cached_file_name):\n",
    "        vocab, training_data, valid_data, test_data = load_cached_data(cached_file_name)\n",
    "        data = training_data + valid_data + test_data\n",
    "        labels = []\n",
    "        for (feature, l, _) in data:\n",
    "            labels.extend(l)\n",
    "        unique_labels=list(set(labels))\n",
    "        n_labels = len(unique_labels)\n",
    "        labels = unique_labels\n",
    "        vocab.update_labels(labels)\n",
    "        print(\"Loaded vocab and data from file\")\n",
    "    else:\n",
    "        training_data = read_data( data_dir + \"/train.csv\")\n",
    "        valid_data = read_data( data_dir + \"/dev.csv\")\n",
    "        test_data = read_data( data_dir + \"/test.csv\")\n",
    "    #     print(len(training_data),len(valid_data),len(test_data))\n",
    "        data = training_data + valid_data + test_data\n",
    "        labels = []\n",
    "        for (feature, l, _) in data:\n",
    "            labels.extend(l)\n",
    "        unique_labels=list(set(labels))\n",
    "        n_labels = len(unique_labels)\n",
    "        labels = unique_labels\n",
    "        vocab = Vocab(data, labels,\n",
    "                          min_word_frequency,\n",
    "                          word_embedding_mode=embedding_mode,\n",
    "                          word_embedding_file=embedding_file)\n",
    "        print(\"Preparing the vocab\")\n",
    "        vocab.prepare_vocab()\n",
    "        saved_objects = {\"vocab\": vocab,\n",
    "                         \"training_data\": training_data,\n",
    "                         \"valid_data\": valid_data,\n",
    "                         \"test_data\": test_data}\n",
    "        with open(cached_file_name, 'wb') as f:\n",
    "            pickle.dump(saved_objects, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "        print(\"Saved vocab and data to files\")\n",
    "    return training_data, valid_data, test_data, vocab, cached_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8dcb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab and data from file\n"
     ]
    }
   ],
   "source": [
    "training_data, valid_data, test_data, vocab, saved_vocab_path = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55dbe24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_mode: str,\n",
    "                 pretrained_word_embeddings: torch.Tensor,\n",
    "                 vocab_size: int,\n",
    "                 embedding_size: int):\n",
    "        \n",
    "        self.embedding_mode = embedding_mode\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings.weight = nn.Parameter(copy.deepcopy(pretrained_word_embeddings), requires_grad=False)\n",
    "        self.output_size = embedding_size\n",
    "        \n",
    "    def forward(self, batch_data: torch.LongTensor): # batch_data shape: [batch_size x max_padded_text_length]\n",
    "        embeds = self.embeddings(batch_data)  # [batch_size x max_seq_size x embedding_size]\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2f6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, size: int, n_labels=None):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.d_a = d_a\n",
    "        self.n_labels = n_labels\n",
    "        self.linear = nn.Linear(self.size, self.n_labels, bias = True)\n",
    "        self._init_weights(mean=0.0, std=0.03)\n",
    "\n",
    "    def _init_weights(self, mean=0.0, std=0.03) -> None:\n",
    "        torch.nn.init.normal_(self.linear.weight,mean, std)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_output = self.linear(x)   \n",
    "        return weighted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46f47efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab: Vocab):\n",
    "        super(Model, self).__init__()\n",
    "        self.vocab_size = vocab.n_words()\n",
    "        self.vocab = vocab\n",
    "        self.use_last_hidden_state = use_last_hidden_state\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bool(bidirectional)\n",
    "        self.n_directions = int(self.bidirectional) + 1\n",
    "        self.output_size = self.hidden_size * self.n_directions\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.embedding = EmbeddingLayer(embedding_mode=embedding_mode,\n",
    "                                     embedding_size=embedding_size,\n",
    "                                     pretrained_word_embeddings=vocab.word_embeddings,\n",
    "                                     vocab_size=vocab.n_words())\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.embedding.output_size, self.hidden_size, num_layers=self.n_layers,\n",
    "                               bidirectional=self.bidirectional, dropout=(self.dropout if self.n_layers > 1 else 0))\n",
    "        \n",
    "        self.use_dropout = dropout > 0\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.linear = LinearLayer(size=self.output_size, n_labels=self.vocab.n_labels())\n",
    "\n",
    "    def init_hidden(self, batch_size) -> Variable:\n",
    "        h = Variable(torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)).to(device)\n",
    "        c = Variable(torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)).to(device)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, batch_data: torch.LongTensor, lengths: torch.LongTensor) -> tuple:\n",
    "        batch_size = batch_data.size()[0]\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.embedding(batch_data)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            embeds = self.dropout(embeds)\n",
    "            \n",
    "        self.rnn.flatten_parameters()\n",
    "        embeds = pack_padded_sequence(embeds, lengths.cpu(), batch_first=True)\n",
    "        rnn_output, hidden = self.rnn(embeds, hidden) \n",
    "        hidden = hidden[0]\n",
    "        rnn_output = pad_packed_sequence(rnn_output)[0]\n",
    "        rnn_output = rnn_output.permute(1, 0, 2)\n",
    "        \n",
    "        hidden_forward = hidden[-1]\n",
    "        hidden_backward = hidden[0]        \n",
    "        last_rnn_output = torch.cat((hidden_forward, hidden_backward), 1)\n",
    "        \n",
    "        output = self.linear(last_rnn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a79faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1d85c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (embeddings): Embedding(66322, 100)\n",
       "  )\n",
       "  (rnn): LSTM(100, 256, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): LinearLayer(\n",
       "    (linear): Linear(in_features=512, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(vocab)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c79c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data: list, vocab, sort: bool = True):\n",
    "        super(TextDataset, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.multilabel = multilabel\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.min_seq_length = min_seq_length\n",
    "        self.PAD_ID = self.vocab.index_of_word(self.vocab.PAD_TOKEN) # 0\n",
    "        indexed_data = []\n",
    "        self.n_instances = len(text_data)\n",
    "        print('Length of text data:- ', self.n_instances)\n",
    "        self.n_total_tokens = 0\n",
    "\n",
    "        n_label_level = 1\n",
    "        self.label_count = dict()\n",
    "        self.labels = set()\n",
    "        for text, labels, _id in tqdm.tqdm(text_data, unit=\"samples\", desc=\"Processing data\"):\n",
    "            label_list = []\n",
    "            for label in labels:\n",
    "                if label in self.vocab.label2index:\n",
    "                    label = self.vocab.index_of_label(label)\n",
    "                    if label not in self.label_count:\n",
    "                        self.label_count[label] = 1\n",
    "                    else:\n",
    "                        self.label_count[label] += 1\n",
    "                    self.labels.add(label)\n",
    "                    label_list.append(label)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if len(label_list) == 0:\n",
    "                continue\n",
    "\n",
    "            word_seq = []\n",
    "            sent_words = text.strip().split()\n",
    "#             print('sent words:- ', len(sent_words))\n",
    "            for word in sent_words:\n",
    "                word_idx = vocab.index_of_word(word)\n",
    "#                 if word_idx==vocab.word2index['_UNK']:\n",
    "#                     continue\n",
    "                word_seq.append(word_idx)\n",
    "                self.n_total_tokens += 1\n",
    "                if len(word_seq) >= self.max_seq_length:\n",
    "                    break\n",
    "            if len(word_seq) > 0:\n",
    "                indexed_data.append((word_seq, label_list, _id))\n",
    "#             print('word sequence:- ', len(word_seq))\n",
    "\n",
    "        if sort:\n",
    "            self.indexed_data = sorted(indexed_data, key=lambda x: -len(x[0]))\n",
    "        else:\n",
    "            self.indexed_data = indexed_data\n",
    "            self.shuffle_data()\n",
    "\n",
    "        self.labels = sorted(list(self.labels))\n",
    "        self.size = len(self.indexed_data)\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        random.shuffle(self.indexed_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word_seq, label_list, _id = self.indexed_data[index]\n",
    "        if len(word_seq) > self.max_seq_length:\n",
    "            word_seq = word_seq[:self.max_seq_length]\n",
    "\n",
    "        one_hot_label_list = [0] * self.vocab.n_labels()\n",
    "        for label in label_list:\n",
    "            one_hot_label_list[label] = 1\n",
    "        return word_seq, one_hot_label_list, _id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b54069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_and_cache_data(train_data, valid_data, test_data, vocab, saved_data_file_path):\n",
    "    if os.path.exists(saved_data_file_path):\n",
    "        try:\n",
    "            with open(saved_data_file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                data[\"train\"].multilabel = bool(multilabel)\n",
    "                data[\"valid\"].multilabel = bool(multilabel)\n",
    "                data[\"test\"].multilabel = bool(multilabel)\n",
    "\n",
    "                return data[\"train\"], data[\"valid\"], data[\"test\"]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Build train/valid/test data loaders\n",
    "    train_dataset = TextDataset(train_data, vocab,sort=True)\n",
    "\n",
    "    valid_dataset = TextDataset(valid_data, vocab,sort=True)\n",
    "\n",
    "    test_dataset = TextDataset(test_data, vocab, sort=True)\n",
    "    # try:\n",
    "    with open(saved_data_file_path, 'wb') as f:\n",
    "        data = {\"train\": train_dataset, \"valid\": valid_dataset, \"test\": test_dataset}\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8167d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataLoader(DataLoader):\n",
    "    def __init__(self, vocab, **kwargs):\n",
    "        super(TextDataLoader, self).__init__( **kwargs)\n",
    "        self.collate_fn = self._collate_fn # recieves is a list of tuples of (word seq, one_hot_label_list, _id)\n",
    "        self.PAD_ID = vocab.index_of_word(vocab.PAD_TOKEN)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "#         print('collate_fn called:- ')\n",
    "#         print(len(batch[0][0]))\n",
    "        length_batch = []\n",
    "\n",
    "#         self.first_linear = nn.Linear(self.size, self.d_a, bias=False)\n",
    "#         self.second_linear = nn.Linear(self.d_a, n_labels, bias=False)\n",
    "#         self.third_linear = nn.Linear(self.size , n_labels, bias=True)\n",
    "        feature_batch = []\n",
    "        label_batch = []\n",
    "\n",
    "        id_batch = []\n",
    "        multilabel = True\n",
    "        for features, labels, _id in batch: # labels are one-hot encoded\n",
    "            feature_length = len(features)\n",
    "            feature_batch.append(torch.LongTensor(features))\n",
    "\n",
    "            length_batch.append(feature_length)\n",
    "            label_batch.append(labels)\n",
    "            id_batch.append(_id)\n",
    "\n",
    "        feature_batch, label_batch, length_batch, id_batch = \\\n",
    "            self.sort_batch(feature_batch, label_batch, length_batch, id_batch)\n",
    "        \n",
    "        padded_batch = pad_sequence(feature_batch, batch_first=True)\n",
    "        feature_batch = torch.LongTensor(padded_batch)\n",
    "        \n",
    "        label_batch = np.stack(label_batch, axis=0)\n",
    "        \n",
    "        label_batch = torch.FloatTensor(label_batch.tolist()) # converts list to tensor\n",
    "#         print('before longtensor length batch',length_batch)\n",
    "        length_batch = torch.LongTensor(length_batch)\n",
    "#         print('after longtensor length batch',length_batch)\n",
    "\n",
    "#         print('shape of feature batch:-', feature_batch.shape)\n",
    "#         print('length batch:-', length_batch)\n",
    "#         print('shape of label batch:-', label_batch.shape)\n",
    "#         print('id batch:-', id_batch)\n",
    "#         print('type of length batch:- ', type(length_batch))\n",
    "        return feature_batch, label_batch, length_batch, id_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_batch(features, labels, lengths, id_batch):\n",
    "        sorted_indices = sorted(range(len(features)), key=lambda i: features[i].size(0), reverse=True)\n",
    "        sorted_features = []\n",
    "        sorted_labels = []\n",
    "        sorted_lengths = []\n",
    "        sorted_ids = []\n",
    "\n",
    "        for index in sorted_indices:\n",
    "            sorted_features.append(features[index])\n",
    "\n",
    "            sorted_labels.append(labels[index])\n",
    "            sorted_lengths.append(lengths[index])\n",
    "\n",
    "            sorted_ids.append(id_batch[index])\n",
    "\n",
    "        return sorted_features, sorted_labels, sorted_lengths, sorted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f66ab735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self,model: torch.nn.Module, vocab, criterions, n_training_labels):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.index_to_label = vocab.index2label\n",
    "        self.multilabel = multilabel\n",
    "        self.criterions = criterions\n",
    "        self.n_training_labels = n_training_labels\n",
    "\n",
    "    def evaluate(self, dataloader: TextDataLoader) -> dict:\n",
    "\n",
    "        self.model.eval()\n",
    "        pred_probs = []\n",
    "        true_labels = []\n",
    "        ids = []\n",
    "\n",
    "        losses = []\n",
    "        all_loss_list = []\n",
    "\n",
    "        for text_batch, label_batch, length_batch, id_batch in tqdm.tqdm(dataloader, unit=\"batches\", desc=\"Evaluating\"):\n",
    "\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            length_batch = length_batch.to(device)\n",
    "            \n",
    "            true_labels.extend(label_batch.cpu().numpy())\n",
    "            ids.extend(id_batch)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(text_batch, length_batch)\n",
    "#                 output, attn_weights = self.model(text_batch, length_batch)\n",
    "\n",
    "            probs = [None] * len(output)\n",
    "            \n",
    "            loss = self.criterions(output, label_batch)\n",
    "            output = torch.sigmoid(output)\n",
    "            all_loss_list.append(loss.item())\n",
    "            output = output.detach().cpu().numpy()\n",
    "            probs = output.tolist()\n",
    "            pred_probs.extend(output)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        scores = OrderedDict()\n",
    "        scores = calculate_eval_metrics(ids, true_labels, pred_probs, self.multilabel)\n",
    "        scores[\"loss\"] = np.mean(all_loss_list).item()\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0075522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eval_metrics(ids, true_labels, pred_probs, is_multilabel):\n",
    "    true_labels = np.asarray(true_labels)\n",
    "    pred_probs = np.asarray(pred_probs)\n",
    "    pred_labels = np.rint(pred_probs)\n",
    "    \n",
    "    macro_scores = calculate_scores(true_labels, pred_labels, pred_probs, \"macro\", is_multilabel)\n",
    "    micro_scores = calculate_scores(true_labels, pred_labels, pred_probs, \"micro\", is_multilabel)\n",
    "    \n",
    "    scores = macro_scores\n",
    "    scores.update(micro_scores)\n",
    "    scores[\"ids\"] = ids\n",
    "    scores[\"true_labels\"] = true_labels\n",
    "    scores[\"pred_probs\"] = pred_probs\n",
    "    return scores\n",
    "\n",
    "def calculate_scores(true_labels, pred_labels, pred_probs, average=\"macro\", is_multilabel=True):\n",
    "    max_size = min(len(true_labels), len(pred_labels))\n",
    "    true_labels = true_labels[: max_size]\n",
    "    pred_labels = pred_labels[: max_size]\n",
    "    pred_probs = pred_probs[: max_size]\n",
    "    p_1 = 0\n",
    "    p_5 = 0\n",
    "    p_8 = 0\n",
    "    p_10 = 0\n",
    "    p_15 = 0\n",
    "    if pred_probs is not None:\n",
    "        if average == \"macro\":\n",
    "            accuracy = macro_accuracy(true_labels, pred_labels)  # categorical accuracy\n",
    "            precision, recall, f1 = macro_f1(true_labels, pred_labels)\n",
    "            p_ks = precision_at_k(true_labels, pred_probs, [1, 5, 8, 10, 15])\n",
    "            p_1 = p_ks[0]\n",
    "            p_5 = p_ks[1]\n",
    "            p_8 = p_ks[2]\n",
    "            p_10 = p_ks[3]\n",
    "            p_15 = p_ks[4]\n",
    "\n",
    "        else:\n",
    "            accuracy = micro_accuracy(true_labels, pred_labels)\n",
    "            precision, recall, f1 = micro_f1(true_labels, pred_labels)\n",
    "        auc_score = roc_auc(true_labels, pred_probs, average)\n",
    "    else:\n",
    "        auc_score = -1\n",
    "\n",
    "    output = {\"{}_precision\".format(average): precision, \"{}_recall\".format(average): recall,\n",
    "              \"{}_f1\".format(average): f1, \"{}_accuracy\".format(average): accuracy,\n",
    "              \"{}_auc\".format(average): auc_score, \"{}_P@1\".format(average): p_1, \"{}_P@5\".format(average): p_5,\n",
    "              \"{}_P@8\".format(average): p_8, \"{}_P@10\".format(average): p_10, \"{}_P@15\".format(average): p_15}\n",
    "    \n",
    "    return output\n",
    "\n",
    "def average_scores(scores):\n",
    "    avg_scores = dict()\n",
    "    for key in scores:\n",
    "        if key.startswith(\"level\"):\n",
    "            for metric in scores[key]:\n",
    "                if metric.startswith(\"macro\") or metric.startswith(\"micro\") or metric == \"loss\":\n",
    "                    if metric not in avg_scores:\n",
    "                        avg_scores[metric] = []\n",
    "                    avg_scores[metric].append(scores[key][metric])\n",
    "\n",
    "    for metric in avg_scores:\n",
    "        avg_scores[metric] = sum(avg_scores[metric]) / len(avg_scores[metric])\n",
    "    return avg_scores\n",
    "\n",
    "def union_size(x, y, axis):\n",
    "    return np.logical_or(x, y).sum(axis=axis).astype(float)\n",
    "\n",
    "\n",
    "def intersect_size(x, y, axis):\n",
    "    return np.logical_and(x, y).sum(axis=axis).astype(float)\n",
    "\n",
    "def macro_precision(true_labels, pred_labels):\n",
    "    num = intersect_size(true_labels, pred_labels, 0) / (pred_labels.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "\n",
    "def macro_recall(true_labels, pred_labels):\n",
    "    num = intersect_size(true_labels, pred_labels, 0) / (true_labels.sum(axis=0) + 1e-10)\n",
    "#     print('macro recall num:- ', num)\n",
    "    return np.mean(num)\n",
    "\n",
    "\n",
    "def macro_f1(true_labels, pred_labels):\n",
    "    prec = macro_precision(true_labels, pred_labels)\n",
    "    rec = macro_recall(true_labels, pred_labels)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "\n",
    "def macro_accuracy(true_labels, pred_labels):\n",
    "    num = intersect_size(true_labels, pred_labels, 0) / (union_size(true_labels, pred_labels, 0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "\n",
    "def micro_precision(true_labels, pred_labels):\n",
    "    flat_true = true_labels.ravel()\n",
    "    flat_pred = pred_labels.ravel()\n",
    "    if flat_pred.sum(axis=0) == 0:\n",
    "        return 0.0\n",
    "    return intersect_size(flat_true, flat_pred, 0) / flat_pred.sum(axis=0)\n",
    "\n",
    "\n",
    "def micro_recall(true_labels, pred_labels):\n",
    "    flat_true = true_labels.ravel()\n",
    "    flat_pred = pred_labels.ravel()\n",
    "    return intersect_size(flat_true, flat_pred, 0) / flat_true.sum(axis=0)\n",
    "\n",
    "def micro_f1(true_labels, pred_labels):\n",
    "    prec = micro_precision(true_labels, pred_labels)\n",
    "    rec = micro_recall(true_labels, pred_labels)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "\n",
    "def micro_accuracy(true_labels, pred_labels):\n",
    "    flat_true = true_labels.ravel()\n",
    "    flat_pred = pred_labels.ravel()\n",
    "    return intersect_size(flat_true, flat_pred, 0) / union_size(flat_true, flat_pred, 0)\n",
    "\n",
    "\n",
    "def recall_at_k(true_labels, pred_probs, k):\n",
    "    # num true labels in top k predictions / num true labels\n",
    "    sortd = np.argsort(pred_probs)[:, ::-1]\n",
    "    topk = sortd[:, :k]\n",
    "\n",
    "    # get recall at k for each example\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        num_true_in_top_k = true_labels[i, tk].sum()\n",
    "        denom = true_labels[i, :].sum()\n",
    "        vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "    vals = np.array(vals)\n",
    "    vals[np.isnan(vals)] = 0.\n",
    "\n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def precision_at_k(true_labels, pred_probs, ks=[1, 5, 8, 10, 15]):\n",
    "    # num true labels in top k predictions / k\n",
    "    sorted_pred = np.argsort(pred_probs)[:, ::-1]\n",
    "    output = []\n",
    "    for k in ks:\n",
    "        topk = sorted_pred[:, :k]\n",
    "\n",
    "        # get precision at k for each example\n",
    "        vals = []\n",
    "        for i, tk in enumerate(topk):\n",
    "            if len(tk) > 0:\n",
    "                num_true_in_top_k = true_labels[i, tk].sum()\n",
    "                denom = len(tk)\n",
    "                vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "        output.append(np.mean(vals))\n",
    "    return output\n",
    "\n",
    "\n",
    "def roc_auc(true_labels, pred_probs, average=\"macro\"):\n",
    "    if pred_probs.shape[0] <= 1:\n",
    "        return\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    if average == \"macro\":\n",
    "        # get AUC for each label individually\n",
    "        relevant_labels = []\n",
    "        auc_labels = {}\n",
    "        for i in range(true_labels.shape[1]):\n",
    "            # only if there are true positives for this label\n",
    "            if true_labels[:, i].sum() > 0:\n",
    "                fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], pred_probs[:, i])\n",
    "                if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
    "                    auc_score = auc(fpr[i], tpr[i])\n",
    "                    if not np.isnan(auc_score):\n",
    "                        auc_labels[\"auc_%d\" % i] = auc_score\n",
    "                        relevant_labels.append(i)\n",
    "\n",
    "        # macro-AUC: just average the auc scores\n",
    "        aucs = []\n",
    "        for i in relevant_labels:\n",
    "            aucs.append(auc_labels['auc_%d' % i])\n",
    "        score = np.mean(aucs)\n",
    "    else:\n",
    "        # micro-AUC: just look at each individual prediction\n",
    "        flat_pred = pred_probs.ravel()\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true_labels.ravel(), flat_pred)\n",
    "        score = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def multiclass_roc_auc(true_labels, pred_labels, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(true_labels)\n",
    "    true_labels = lb.transform(true_labels)\n",
    "    pred_labels = lb.transform(pred_labels)\n",
    "    return roc_auc_score(true_labels, pred_labels, average=average)\n",
    "\n",
    "def average_scores(scores):\n",
    "    avg_scores = dict()\n",
    "    for key in scores:\n",
    "        if key.startswith(\"level\"):\n",
    "            for metric in scores[key]:\n",
    "                if metric.startswith(\"macro\") or metric.startswith(\"micro\") or metric == \"loss\":\n",
    "                    if metric not in avg_scores:\n",
    "                        avg_scores[metric] = []\n",
    "                    avg_scores[metric].append(scores[key][metric])\n",
    "\n",
    "    for metric in avg_scores:\n",
    "        avg_scores[metric] = sum(avg_scores[metric]) / len(avg_scores[metric])\n",
    "    return avg_scores\n",
    "\n",
    "def normalise_labels(labels, n_label):\n",
    "    norm_labels = []\n",
    "    for label in labels:\n",
    "        one_hot_vector_label = [0] * n_label\n",
    "        one_hot_vector_label[label] = 1\n",
    "        norm_labels.append(one_hot_vector_label)\n",
    "    return np.asarray(norm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e2041d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 train_dataloader: TextDataLoader,\n",
    "                 valid_dataloader: TextDataLoader,\n",
    "                 test_dataloader: TextDataLoader,\n",
    "                 criterions,\n",
    "                 optimiser,\n",
    "#                  lr_scheduler,\n",
    "                 vocab,\n",
    "                 checkpoint_path,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.criterions = criterions\n",
    "        self.optimiser = optimiser\n",
    "#         self.lr_scheduler = lr_scheduler\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.multilabel = multilabel\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.n_training_labels = n_labels\n",
    "        self.main_metric = main_metric\n",
    "        self.start_epoch = 0\n",
    "\n",
    "    def train_single_epoch(self, index):\n",
    "        self.model.train()\n",
    "        self.train_dataloader.dataset.shuffle_data()\n",
    "        losses = []\n",
    "        true_labels = []\n",
    "        pred_probs = []\n",
    "        ids = []\n",
    "        all_loss_list = []\n",
    "        progress_bar = tqdm.tqdm(self.train_dataloader, unit=\"batches\", desc=\"Training at epoch #{}\".format(index))\n",
    "        progress_bar.clear()\n",
    "        self.optimiser.zero_grad()\n",
    "        batch_id = 0\n",
    "\n",
    "        for text_batch, label_batch, length_batch, id_batch in progress_bar:\n",
    "            batch_id += 1\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            length_batch = length_batch.to(device)\n",
    "\n",
    "#             output, attn_weights = self.model(text_batch, length_batch)\n",
    "            \n",
    "            output = self.model(text_batch, length_batch)\n",
    "            true_labels.extend(label_batch.cpu().numpy())\n",
    "\n",
    "            ids.extend(id_batch)\n",
    "\n",
    "            loss = self.criterions(output, label_batch)\n",
    "            output = torch.sigmoid(output)\n",
    "            all_loss_list.append(loss.item())\n",
    "            output = output.detach().cpu().numpy()\n",
    "            pred_probs.extend(output)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "#             print('pred probs:- ', output)\n",
    "#             print('true labels:- ', label_batch)\n",
    "\n",
    "            self.optimiser.step()\n",
    "#             self.lr_scheduler.step()\n",
    "            self.optimiser.zero_grad()\n",
    "        \n",
    "#         print('pred labels', pred_probs[0:3])\n",
    "\n",
    "        scores = OrderedDict()\n",
    "        scores = calculate_eval_metrics(ids, true_labels, pred_probs, self.multilabel)\n",
    "        scores[\"loss\"] = np.mean(all_loss_list).item()\n",
    "\n",
    "        progress_bar.refresh(True)\n",
    "        progress_bar.clear(True)\n",
    "        progress_bar.close()\n",
    "#         print('lr {}'.format(self.optimiser.param_groups[0]['lr']))\n",
    "        print('loss:- ', scores['loss'], '  micro auc:- ', scores['micro_auc'], '  macro auc:- ', scores['macro_auc'])\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def format_number(number):\n",
    "        return abs(round(number, ndigits=ndigits))\n",
    "\n",
    "    def train(self, n_epoch: int = 100):\n",
    "        evaluator = Evaluator(self.model, self.vocab, self.criterions, self.n_training_labels)\n",
    "        for e in range(self.start_epoch + 1, n_epoch + 1):\n",
    "            train_scores = self.train_single_epoch(e)\n",
    "            valid_scores = evaluator.evaluate(self.valid_dataloader)\n",
    "            print('loss:- ', valid_scores['loss'], '  micro auc:- ', valid_scores['micro_auc'], '  macro auc:- ', valid_scores['macro_auc'])\n",
    "        test_scores = evaluator.evaluate(self.test_dataloader)\n",
    "        return self.model, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee0e163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model(train_data, valid_data, test_data, vocab, saved_data_file_path=None,checkpoint_path=None):\n",
    "    model = Model(vocab)\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset = _load_and_cache_data(train_data, valid_data, test_data,\n",
    "                                                                      vocab, saved_data_file_path)\n",
    "    train_dataloader = TextDataLoader(dataset=train_dataset, vocab=vocab, batch_size=BATCH_SIZE)\n",
    "\n",
    "    valid_dataloader = TextDataLoader(dataset=valid_dataset, vocab=vocab, batch_size=BATCH_SIZE)\n",
    "\n",
    "    test_dataloader = TextDataLoader(dataset=test_dataset, vocab=vocab, batch_size=BATCH_SIZE)\n",
    "\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "#     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=50)\n",
    "    \n",
    "    criterions = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(8))\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      valid_dataloader=valid_dataloader,\n",
    "                      test_dataloader=test_dataloader,\n",
    "                      criterions=criterions,\n",
    "                      optimiser=optimiser,\n",
    "#                       lr_scheduler = lr_scheduler,\n",
    "                      vocab=vocab,\n",
    "                      checkpoint_path=checkpoint_path)\n",
    "    best_model, scores = trainer.train(n_epoch=n_epoch)\n",
    "\n",
    "    evaluator = Evaluator(model=best_model,vocab=vocab,criterions=criterions,\n",
    "                          n_training_labels=n_labels)\n",
    "\n",
    "    del model, optimiser, evaluator, trainer, criterions\n",
    "    return best_model, scores  # either on valid or test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13164513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_validation(training_data, valid_data, test_data, vocab, saved_data_file_path):\n",
    "    best_model, scores = _train_model(\n",
    "        train_data=training_data, valid_data=valid_data, test_data=test_data,\n",
    "        vocab=vocab, saved_data_file_path=saved_data_file_path, checkpoint_path=\"../model/checkpoint.pkl\")\n",
    "\n",
    "    return best_model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "772dacdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, test_scores \u001b[38;5;241m=\u001b[39m \u001b[43mrun_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_data_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.data.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_vocab_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [25], line 2\u001b[0m, in \u001b[0;36mrun_with_validation\u001b[0;34m(training_data, valid_data, test_data, vocab, saved_data_file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_validation\u001b[39m(training_data, valid_data, test_data, vocab, saved_data_file_path):\n\u001b[0;32m----> 2\u001b[0m     best_model, scores \u001b[38;5;241m=\u001b[39m \u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_data_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaved_data_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../model/checkpoint.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model, scores\n",
      "Cell \u001b[0;32mIn [24], line 3\u001b[0m, in \u001b[0;36m_train_model\u001b[0;34m(train_data, valid_data, test_data, vocab, saved_data_file_path, checkpoint_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_model\u001b[39m(train_data, valid_data, test_data, vocab, saved_data_file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m Model(vocab)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     train_dataset, valid_dataset, test_dataset \u001b[38;5;241m=\u001b[39m _load_and_cache_data(train_data, valid_data, test_data,\n\u001b[1;32m      6\u001b[0m                                                                       vocab, saved_data_file_path)\n\u001b[1;32m      7\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m TextDataLoader(dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, vocab\u001b[38;5;241m=\u001b[39mvocab, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:612\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:359\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 359\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    370\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:381\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 381\u001b[0m         param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:610\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal"
     ]
    }
   ],
   "source": [
    "model, test_scores = run_with_validation(training_data, valid_data, test_data, vocab, saved_data_file_path=\"{}.data.pkl\".format(saved_vocab_path.split(\".pkl\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "89879854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro auc:-  0.8621733895126819\n",
      "macro auc:-  0.8143088123516844\n",
      "\n",
      "macro F1:-  0.43694100355330756\n",
      "micro_F1:-  0.4430170575692963\n",
      "\n",
      "P@5:-  0.4875650665124349\n",
      "P@8:-  0.4050751879699248\n",
      "P@15:-  0.2980913823019087\n"
     ]
    }
   ],
   "source": [
    "print('micro auc:- ', test_scores['micro_auc'])\n",
    "print('macro auc:- ', test_scores['macro_auc'])\n",
    "print('\\nmacro F1:- ', test_scores['macro_f1'])\n",
    "print('micro_F1:- ', test_scores['micro_f1'])\n",
    "print('\\nP@5:- ', test_scores['macro_P@5'])\n",
    "print('P@8:- ', test_scores['macro_P@8'])\n",
    "print('P@15:- ', test_scores['macro_P@15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e88dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
