{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71afa6b7",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6a9cb6ed",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import tqdm.notebook as tqdm\n",
    "from torch import optim\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import *\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d7cbab24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ac72555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir =  \"../cache\"\n",
    "data_dir = \"../data/mimic3/50\"\n",
    "id_col_name = \"HADM_ID\"\n",
    "label_col_name = \"ICD10_CODE\"\n",
    "text_col_name= \"TEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a75cd2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 20 19:52:51 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  Off  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   50C    P0    56W / 300W |    775MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  Off  | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   50C    P0    55W / 300W |   9239MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  Off  | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   49C    P0    57W / 300W |   1002MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  Off  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   52C    P0    70W / 300W |  19732MiB / 32508MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    186375      C   /usr/bin/python3                  465MiB |\n",
      "|    0   N/A  N/A    186917      C   /usr/bin/python3                  305MiB |\n",
      "|    1   N/A  N/A    186375      C   /usr/bin/python3                 6315MiB |\n",
      "|    1   N/A  N/A    187912      C   /usr/bin/python3                 2919MiB |\n",
      "|    2   N/A  N/A    186917      C   /usr/bin/python3                  999MiB |\n",
      "|    3   N/A  N/A    186917      C   /usr/bin/python3                19729MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69129c7b",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d7afed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "n_epoch=50\n",
    "optimiser=\"adamw\" # \"adam\", \"sgd\", \"adadelta\", \"adamw\",\"adagrad\"\n",
    "joint_mode=\"flat\"\n",
    "main_metric=\"micro_f1\"\n",
    "multilabel=1\n",
    "shuffle_data=1\n",
    "dropout=0.3\n",
    "max_seq_length=4000\n",
    "min_seq_length=-1\n",
    "min_word_frequency=-1\n",
    "embedding_mode=\"word2vec\"\n",
    "embedding_size=100\n",
    "embedding_file=\"../data/mimic3/processed_full.embed\"\n",
    "attention_mode = \"label\" # \"hard\", \"self\", \"label\", \"caml\"\n",
    "d_a=256 #\"The dimension of the first dense layer for self attention\"\n",
    "r = -1  #The number of hops for self attention\n",
    "n_labels=50\n",
    "\n",
    "#RNN\n",
    "hidden_size =  256\n",
    "n_layers = 1\n",
    "bidirectional =1\n",
    "use_last_hidden_state=0\n",
    "rnn_model=\"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6d3ef67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self,\n",
    "                 training_data: list,\n",
    "                 training_labels: list,\n",
    "                 min_word_frequency: int = -1,\n",
    "                 max_vocab_size: int = -1,\n",
    "                 word_embedding_mode: str = \"word2vec\",\n",
    "                 word_embedding_file: str = None,\n",
    "                 use_gpu: bool = True\n",
    "                 ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "        self.word_embedding_mode = word_embedding_mode\n",
    "        self.word_embedding_file = word_embedding_file\n",
    "        self.word_embedding_size = 100\n",
    "        self.word_embeddings = None\n",
    "\n",
    "        self.training_data = training_data\n",
    "\n",
    "        self.PAD_TOKEN = '_PAD'\n",
    "        self.UNK_TOKEN = '_UNK'\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "\n",
    "        self.label2index = None\n",
    "        self.index2label = None\n",
    "\n",
    "        self.vocab_words = [self.PAD_TOKEN, self.UNK_TOKEN]\n",
    "        self.all_labels = []\n",
    "\n",
    "        self.min_word_frequency = min_word_frequency\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "        self.update_labels(training_labels)\n",
    "\n",
    "    def index_of_word(self,word: str) -> int:\n",
    "        try:\n",
    "            return self.word2index[word]\n",
    "        except:\n",
    "            return self.word2index[self.UNK_TOKEN]\n",
    "\n",
    "    def index_of_label(self,label: str) -> int:\n",
    "        try:\n",
    "            return self.label2index[label]\n",
    "        except:\n",
    "            return 0\n",
    "    def update_labels(self, labels):\n",
    "        self.all_labels = []\n",
    "        self.index2label = []\n",
    "        self.label2index = []\n",
    "        all_labels = list(sorted(labels))\n",
    "        self.label2index = {label: idx for idx, label in enumerate(all_labels)}\n",
    "        self.index2label = {idx: label for idx, label in enumerate(all_labels)}\n",
    "        self.all_labels = all_labels\n",
    "            \n",
    "    def prepare_vocab(self):\n",
    "        self._build_vocab()\n",
    "\n",
    "        # load pretrain word embeddings\n",
    "        if self.word_embedding_file is not None:\n",
    "            self.word_embeddings = torch.FloatTensor(self._load_embeddings())\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        all_words_df = pd.read_csv('../data/mimic3/vocab.csv',header=None)\n",
    "        all_words = all_words_df.iloc[:,0].tolist()\n",
    "        all_words.sort()\n",
    "\n",
    "        self.vocab_words += all_words\n",
    "\n",
    "        self.word2index = {word: idx for idx, word in enumerate(self.vocab_words)}\n",
    "        self.index2word = {idx: word for idx, word in enumerate(self.vocab_words)}\n",
    "\n",
    "\n",
    "    def _load_embeddings(self):\n",
    "        if self.word_embedding_file is None:\n",
    "            return None\n",
    "        return self._load_word_embeddings()\n",
    "\n",
    "    def _load_word_embeddings(self):\n",
    "        unknown_vec = np.random.uniform(-0.25, 0.25, self.word_embedding_size)\n",
    "        embeddings = [unknown_vec] * (len(self.vocab_words))\n",
    "        embeddings[0] = np.zeros(self.word_embedding_size)\n",
    "        for line in open(self.word_embedding_file, \"rt\"):\n",
    "            split = line.rstrip().split(\" \")\n",
    "            word = split[0]\n",
    "            vector = np.array([float(num) for num in split[1:]]).astype(np.float32)\n",
    "#             print(word,': ',vector)\n",
    "            if len(vector) > 0:\n",
    "                if word in self.word2index:\n",
    "                    embeddings[self.word2index[word]] = vector\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "        return embeddings\n",
    "    def n_words(self):\n",
    "        return len(self.vocab_words)\n",
    "\n",
    "    def n_labels(self):\n",
    "        return len(self.all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fc5dfc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path) -> list:\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    id_data = data[id_col_name]\n",
    "    text_data = data[text_col_name]\n",
    "\n",
    "    hierarchical_label_data = []\n",
    "    label_data = data[label_col_name].tolist()\n",
    "    \n",
    "    output = []\n",
    "    for i in tqdm.tqdm(range(len(label_data)), desc=\"Reading data\"):\n",
    "        labels = label_data[i].split(';')\n",
    "        output.append((text_data[i], labels, id_data[i]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a07b35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cached_data(file_path: str) -> tuple:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        f.close()\n",
    "    return data[\"vocab\"], data[\"training_data\"], data[\"valid_data\"], data[\"test_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3f6718ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    cache_folder = \"{}/{}\".format(cache_dir, \"mimic3_single_50\")\n",
    "#     device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    save_vocab_file_name = \"{}.pkl\".format(\"save_vocab\")\n",
    "    cached_file_name = os.path.join(cache_folder, save_vocab_file_name)\n",
    "    if os.path.exists(cached_file_name):\n",
    "        vocab, training_data, valid_data, test_data = load_cached_data(cached_file_name)\n",
    "        data = training_data + valid_data + test_data\n",
    "        labels = []\n",
    "        for (feature, l, _) in data:\n",
    "            labels.extend(l)\n",
    "        unique_labels=list(set(labels))\n",
    "        n_labels = len(unique_labels)\n",
    "        labels = unique_labels\n",
    "        vocab.update_labels(labels)\n",
    "        print(\"Loaded vocab and data from file\")\n",
    "    else:\n",
    "        training_data = read_data( data_dir + \"/train.csv\")\n",
    "        valid_data = read_data( data_dir + \"/dev.csv\")\n",
    "        test_data = read_data( data_dir + \"/test.csv\")\n",
    "    #     print(len(training_data),len(valid_data),len(test_data))\n",
    "        data = training_data + valid_data + test_data\n",
    "        labels = []\n",
    "        for (feature, l, _) in data:\n",
    "            labels.extend(l)\n",
    "        unique_labels=list(set(labels))\n",
    "        n_labels = len(unique_labels)\n",
    "        labels = unique_labels\n",
    "        vocab = Vocab(data, labels,\n",
    "                          min_word_frequency,\n",
    "                          word_embedding_mode=embedding_mode,\n",
    "                          word_embedding_file=embedding_file)\n",
    "        print(\"Preparing the vocab\")\n",
    "        vocab.prepare_vocab()\n",
    "        saved_objects = {\"vocab\": vocab,\n",
    "                         \"training_data\": training_data,\n",
    "                         \"valid_data\": valid_data,\n",
    "                         \"test_data\": test_data}\n",
    "        with open(cached_file_name, 'wb') as f:\n",
    "            pickle.dump(saved_objects, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "        print(\"Saved vocab and data to files\")\n",
    "    return training_data, valid_data, test_data, vocab, cached_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "cb8dcb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab and data from file\n"
     ]
    }
   ],
   "source": [
    "training_data, valid_data, test_data, vocab, saved_vocab_path = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "55dbe24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_mode: str,\n",
    "                 pretrained_word_embeddings: torch.Tensor,\n",
    "                 vocab_size: int,\n",
    "                 embedding_size: int):\n",
    "        \n",
    "        self.embedding_mode = embedding_mode\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings.weight = nn.Parameter(copy.deepcopy(pretrained_word_embeddings), requires_grad=False)\n",
    "        self.output_size = embedding_size\n",
    "        \n",
    "    def forward(self, batch_data: torch.LongTensor): # batch_data shape: [batch_size x max_padded_text_length]\n",
    "        embeds = self.embeddings(batch_data)  # [batch_size x max_seq_size x embedding_size]\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ca2f6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, size: int, n_labels=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention_mode = attention_mode\n",
    "        self.size = size\n",
    "        self.d_a = d_a\n",
    "        self.n_labels = n_labels\n",
    "        self.first_linear = nn.Linear(self.size, self.d_a, bias=False)\n",
    "        self.second_linear = nn.Linear(self.d_a, n_labels, bias=False)\n",
    "        self.third_linear = nn.Linear(self.size , n_labels, bias=True)\n",
    "        self._init_weights(mean=0.0, std=0.03)\n",
    "\n",
    "    def _init_weights(self, mean=0.0, std=0.03) -> None:\n",
    "        torch.nn.init.normal_(self.first_linear.weight, mean, std)\n",
    "        if self.first_linear.bias is not None:\n",
    "            self.first_linear.bias.data.fill_(0)\n",
    "        torch.nn.init.normal_(self.second_linear.weight, mean, std)\n",
    "        if self.second_linear.bias is not None:\n",
    "            self.second_linear.bias.data.fill_(0)\n",
    "        torch.nn.init.normal_(self.third_linear.weight, mean, std)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print('Input to first Linear Layer:-  ',x.shape)\n",
    "        weights = torch.tanh(self.first_linear(x))\n",
    "#         print('Input to second Linear Layer:- ',weights.shape)\n",
    "        att_weights = self.second_linear(weights)\n",
    "        att_weights = F.softmax(att_weights, 1).transpose(1, 2)\n",
    "        \n",
    "#         print(\"Output from second linear layer:- (A) \", att_weights.shape)\n",
    "        \n",
    "        if len(att_weights.size()) != len(x.size()):\n",
    "            att_weights = att_weights.squeeze()\n",
    "        weighted_output = att_weights @ x\n",
    "        \n",
    "#         print(\"Weighted output (V)\", weighted_output.shape)\n",
    "\n",
    "        if self.attention_mode == \"label\" or self.attention_mode == \"caml\":\n",
    "            batch_size = weighted_output.size(0)\n",
    "            weighted_output = self.third_linear.weight.mul(weighted_output).sum(dim=2).add(self.third_linear.bias)\n",
    "            \n",
    "#         print(\"Output of third linear layer (FFNN): \",weighted_output.shape)\n",
    "        return weighted_output, att_weights\n",
    "\n",
    "    # Using when use_regularisation = True\n",
    "    @staticmethod\n",
    "    def l2_matrix_norm(m):\n",
    "        return torch.sum(torch.sum(torch.sum(m ** 2, 1), 1) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "32615fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_attention_layer(model):\n",
    "    model.attention = AttentionLayer(size=model.output_size, n_labels=model.vocab.n_labels())\n",
    "    model.activation = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "64846a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_attention(model, all_output, last_output):\n",
    "#     print('input to attention model:- ',all_output.shape)\n",
    "    attention_outputs = model.attention(all_output)\n",
    "    weighted_outputs = attention_outputs[0]\n",
    "    attention_weights = attention_outputs[1]\n",
    "#     print('weighted_outputs from attention model:- ', weighted_outputs.shape)\n",
    "#     print('attention_weights from attention model:- ',attention_weights.shape)\n",
    "    return weighted_outputs, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "46f47efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab: Vocab):\n",
    "        super(RNN, self).__init__()\n",
    "        self.vocab_size = vocab.n_words()\n",
    "        self.vocab = vocab\n",
    "        self.use_last_hidden_state = use_last_hidden_state\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bool(bidirectional)\n",
    "        self.n_directions = int(self.bidirectional) + 1\n",
    "        self.attention_mode = attention_mode\n",
    "        self.output_size = self.hidden_size * self.n_directions\n",
    "        self.rnn_model = rnn_model\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.embedding = EmbeddingLayer(embedding_mode=embedding_mode,\n",
    "                                     embedding_size=embedding_size,\n",
    "                                     pretrained_word_embeddings=vocab.word_embeddings,\n",
    "                                     vocab_size=vocab.n_words())\n",
    "        \n",
    "        # embedding.output-size = 100\n",
    "        self.rnn = nn.LSTM(self.embedding.output_size, self.hidden_size, num_layers=self.n_layers,\n",
    "                               bidirectional=self.bidirectional, dropout=(self.dropout if self.n_layers > 1 else 0))\n",
    "        \n",
    "        self.use_dropout = dropout > 0\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # initialize attention layer in RNN model initialization\n",
    "        init_attention_layer(self)\n",
    "\n",
    "    def init_hidden(self, batch_size: int = 8) -> Variable:\n",
    "        # [(n_layers x n_directions) x batch_size x hidden_size]\n",
    "        h = Variable(torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)).to(device)\n",
    "        c = Variable(torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)).to(device)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, batch_data: torch.LongTensor, lengths: torch.LongTensor) -> tuple:\n",
    "        batch_size = batch_data.size()[0]\n",
    "#         print('lstm batch size:- ', batch_size)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.embedding(batch_data)\n",
    "#         \n",
    "#         print('embeddings shape for a batch:- ',embeds.shape)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            embeds = self.dropout(embeds)\n",
    "            \n",
    "        self.rnn.flatten_parameters()\n",
    "        \n",
    "        embeds = pack_padded_sequence(embeds, lengths.cpu(), batch_first=True)\n",
    "\n",
    "        rnn_output, hidden = self.rnn(embeds, hidden)\n",
    "        if self.rnn_model.lower() == \"lstm\":\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        rnn_output = pad_packed_sequence(rnn_output)[0]\n",
    "\n",
    "        rnn_output = rnn_output.permute(1, 0, 2)\n",
    "\n",
    "        weighted_outputs, attention_weights = perform_attention(self, rnn_output, self.get_last_hidden_output(hidden))\n",
    "        activ_outputs = torch.sigmoid(weighted_outputs)\n",
    "        return weighted_outputs, attention_weights\n",
    "\n",
    "    def get_last_hidden_output(self, hidden):\n",
    "        hidden_forward = hidden[-1]\n",
    "        hidden_backward = hidden[0]\n",
    "        if len(hidden_backward.shape) > 2:\n",
    "            hidden_forward = hidden_forward.squeeze(0)\n",
    "            hidden_backward = hidden_backward.squeeze(0)\n",
    "        last_rnn_output = torch.cat((hidden_forward, hidden_backward), 1)\n",
    "        return last_rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e1d85c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): EmbeddingLayer(\n",
       "    (embeddings): Embedding(51919, 100)\n",
       "  )\n",
       "  (rnn): LSTM(100, 256, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (attention): AttentionLayer(\n",
       "    (first_linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "    (second_linear): Linear(in_features=256, out_features=50, bias=False)\n",
       "    (third_linear): Linear(in_features=512, out_features=50, bias=True)\n",
       "  )\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(vocab)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "73d6a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model, input_size = (8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "3c79c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data: list, vocab, sort: bool = True):\n",
    "        super(TextDataset, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.multilabel = multilabel\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.min_seq_length = min_seq_length\n",
    "        self.PAD_ID = self.vocab.index_of_word(self.vocab.PAD_TOKEN) # 0\n",
    "        indexed_data = []\n",
    "        self.n_instances = len(text_data)\n",
    "        print('Length of text data:- ', self.n_instances)\n",
    "        self.n_total_tokens = 0\n",
    "\n",
    "        n_label_level = 1\n",
    "        self.label_count = dict()\n",
    "        self.labels = set()\n",
    "        for text, labels, _id in tqdm.tqdm(text_data, unit=\"samples\", desc=\"Processing data\"):\n",
    "            label_list = []\n",
    "            for label in labels:\n",
    "                if label in self.vocab.label2index:\n",
    "                    label = self.vocab.index_of_label(label)\n",
    "                    if label not in self.label_count:\n",
    "                        self.label_count[label] = 1\n",
    "                    else:\n",
    "                        self.label_count[label] += 1\n",
    "                    self.labels.add(label)\n",
    "                    label_list.append(label)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            if len(label_list) == 0:\n",
    "                continue\n",
    "\n",
    "            word_seq = []\n",
    "            sent_words = text.strip().split()\n",
    "#             print('sent words:- ', len(sent_words))\n",
    "            for word in sent_words:\n",
    "                word_idx = vocab.index_of_word(word)\n",
    "                if word_idx==vocab.word2index['_UNK']:\n",
    "                    continue\n",
    "                word_seq.append(word_idx)\n",
    "                self.n_total_tokens += 1\n",
    "                if len(word_seq) >= self.max_seq_length:\n",
    "                    break\n",
    "            if len(word_seq) > 0:\n",
    "                indexed_data.append((word_seq, label_list, _id))\n",
    "#             print('word sequence:- ', len(word_seq))\n",
    "\n",
    "        if sort:\n",
    "            self.indexed_data = sorted(indexed_data, key=lambda x: -len(x[0]))\n",
    "        else:\n",
    "            self.indexed_data = indexed_data\n",
    "            self.shuffle_data()\n",
    "\n",
    "        self.labels = sorted(list(self.labels))\n",
    "        self.size = len(self.indexed_data)\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        random.shuffle(self.indexed_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word_seq, label_list, _id = self.indexed_data[index]\n",
    "        if len(word_seq) > self.max_seq_length:\n",
    "            word_seq = word_seq[:self.max_seq_length]\n",
    "\n",
    "        one_hot_label_list = [0] * self.vocab.n_labels()\n",
    "        for label in label_list:\n",
    "            one_hot_label_list[label] = 1\n",
    "        return word_seq, one_hot_label_list, _id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5b54069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_and_cache_data(train_data, valid_data, test_data, vocab, saved_data_file_path):\n",
    "    if os.path.exists(saved_data_file_path):\n",
    "        try:\n",
    "            with open(saved_data_file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                data[\"train\"].multilabel = bool(multilabel)\n",
    "                data[\"valid\"].multilabel = bool(multilabel)\n",
    "                data[\"test\"].multilabel = bool(multilabel)\n",
    "\n",
    "                return data[\"train\"], data[\"valid\"], data[\"test\"]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Build train/valid/test data loaders\n",
    "    train_dataset = TextDataset(train_data, vocab,sort=True)\n",
    "\n",
    "    valid_dataset = TextDataset(valid_data, vocab,sort=True)\n",
    "\n",
    "    test_dataset = TextDataset(test_data, vocab, sort=True)\n",
    "    # try:\n",
    "    with open(saved_data_file_path, 'wb') as f:\n",
    "        data = {\"train\": train_dataset, \"valid\": valid_dataset, \"test\": test_dataset}\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8167d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataLoader(DataLoader):\n",
    "    def __init__(self, vocab, **kwargs):\n",
    "        super(TextDataLoader, self).__init__( **kwargs)\n",
    "        self.collate_fn = self._collate_fn # recieves is a list of tuples of (word seq, one_hot_label_list, _id)\n",
    "        self.PAD_ID = vocab.index_of_word(vocab.PAD_TOKEN)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "#         print('collate_fn called:- ')\n",
    "#         print(len(batch[0][0]))\n",
    "        length_batch = []\n",
    "\n",
    "        feature_batch = []\n",
    "        label_batch = []\n",
    "\n",
    "        id_batch = []\n",
    "        multilabel = True\n",
    "        for features, labels, _id in batch: # labels are one-hot encoded\n",
    "            feature_length = len(features)\n",
    "            feature_batch.append(torch.LongTensor(features))\n",
    "\n",
    "            length_batch.append(feature_length)\n",
    "            label_batch.append(labels)\n",
    "            id_batch.append(_id)\n",
    "\n",
    "        feature_batch, label_batch, length_batch, id_batch = \\\n",
    "            self.sort_batch(feature_batch, label_batch, length_batch, id_batch)\n",
    "        \n",
    "        padded_batch = pad_sequence(feature_batch, batch_first=True)\n",
    "        feature_batch = torch.LongTensor(padded_batch)\n",
    "        \n",
    "        label_batch = np.stack(label_batch, axis=0)\n",
    "        \n",
    "        label_batch = torch.FloatTensor(label_batch.tolist()) # converts list to tensor\n",
    "#         print('before longtensor length batch',length_batch)\n",
    "        length_batch = torch.LongTensor(length_batch)\n",
    "#         print('after longtensor length batch',length_batch)\n",
    "\n",
    "#         print('shape of feature batch:-', feature_batch.shape)\n",
    "#         print('length batch:-', length_batch)\n",
    "#         print('shape of label batch:-', label_batch.shape)\n",
    "#         print('id batch:-', id_batch)\n",
    "#         print('type of length batch:- ', type(length_batch))\n",
    "        return feature_batch, label_batch, length_batch, id_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_batch(features, labels, lengths, id_batch):\n",
    "        sorted_indices = sorted(range(len(features)), key=lambda i: features[i].size(0), reverse=True)\n",
    "        sorted_features = []\n",
    "        sorted_labels = []\n",
    "        sorted_lengths = []\n",
    "        sorted_ids = []\n",
    "\n",
    "        for index in sorted_indices:\n",
    "            sorted_features.append(features[index])\n",
    "\n",
    "            sorted_labels.append(labels[index])\n",
    "            sorted_lengths.append(lengths[index])\n",
    "\n",
    "            sorted_ids.append(id_batch[index])\n",
    "\n",
    "        return sorted_features, sorted_labels, sorted_lengths, sorted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f66ab735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self,model: torch.nn.Module, vocab, criterions, n_training_labels):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.index_to_label = vocab.index2label\n",
    "        self.multilabel = multilabel\n",
    "        self.criterions = criterions\n",
    "        self.n_training_labels = n_training_labels\n",
    "\n",
    "    def evaluate(self, dataloader: TextDataLoader) -> dict:\n",
    "\n",
    "        self.model.eval()\n",
    "        pred_probs = []\n",
    "        true_labels = []\n",
    "        ids = []\n",
    "\n",
    "        losses = []\n",
    "        all_loss_list = []\n",
    "\n",
    "        for text_batch, label_batch, length_batch, id_batch in tqdm.tqdm(dataloader, unit=\"batches\", desc=\"Evaluating\"):\n",
    "\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            length_batch = length_batch.to(device)\n",
    "\n",
    "            true_label_batch = []\n",
    "            for idx in range(len(label_batch)):\n",
    "                true_label_batch.append(label_batch[idx].cpu().numpy())\n",
    "            true_labels.extend(true_label_batch)\n",
    "            ids.extend(id_batch)\n",
    "            with torch.no_grad():\n",
    "                output, attn_weights = self.model(text_batch, length_batch)\n",
    "                \n",
    "            true_labels.extend(label_batch.cpu().numpy())\n",
    "            \n",
    "            ids.extend(id_batch)\n",
    "            probs = [None] * len(output)\n",
    "#             output = torch.sigmoid(output)\n",
    "            \n",
    "            loss = self.criterions(output, label_batch)\n",
    "            all_loss_list = [loss.item()]\n",
    "            output = output.detach().cpu().numpy()\n",
    "            probs = output.tolist()\n",
    "            pred_probs.extend(output)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        scores = OrderedDict()\n",
    "        scores = calculate_eval_metrics(ids, true_labels, pred_probs, self.multilabel)\n",
    "        scores[\"loss\"] = np.mean(all_loss_list).item()\n",
    "        scores[\"average\"] = average_scores(scores)\n",
    "        scores[\"average\"][\"loss\"] = np.mean(losses).item()\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0075522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eval_metrics(ids, true_labels, pred_probs, is_multilabel):\n",
    "    true_labels = np.asarray(true_labels)\n",
    "    pred_probs = np.asarray(pred_probs)\n",
    "    pred_labels = np.rint(pred_probs)\n",
    "    \n",
    "    macro_scores = calculate_scores(true_labels, pred_labels, pred_probs, \"macro\", is_multilabel)\n",
    "    micro_scores = calculate_scores(true_labels, pred_labels, pred_probs, \"micro\", is_multilabel)\n",
    "    \n",
    "    scores = macro_scores\n",
    "    scores.update(micro_scores)\n",
    "    scores[\"ids\"] = ids\n",
    "    scores[\"true_labels\"] = true_labels\n",
    "    scores[\"pred_probs\"] = pred_probs\n",
    "    return scores\n",
    "\n",
    "def calculate_scores(true_labels, pred_labels, pred_probs, average=\"macro\", is_multilabel=True):\n",
    "    max_size = min(len(true_labels), len(pred_labels))\n",
    "    true_labels = true_labels[: max_size]\n",
    "    pred_labels = pred_labels[: max_size]\n",
    "    pred_probs = pred_probs[: max_size]\n",
    "    p_1 = 0\n",
    "    p_5 = 0\n",
    "    p_8 = 0\n",
    "    p_10 = 0\n",
    "    p_15 = 0\n",
    "    if pred_probs is not None:\n",
    "        if average == \"macro\":\n",
    "            accuracy = macro_accuracy(true_labels, pred_labels)  # categorical accuracy\n",
    "            precision, recall, f1 = macro_f1(true_labels, pred_labels)\n",
    "            p_ks = precision_at_k(true_labels, pred_probs, [1, 5, 8, 10, 15])\n",
    "            p_1 = p_ks[0]\n",
    "            p_5 = p_ks[1]\n",
    "            p_8 = p_ks[2]\n",
    "            p_10 = p_ks[3]\n",
    "            p_15 = p_ks[4]\n",
    "\n",
    "        else:\n",
    "            accuracy = micro_accuracy(true_labels, pred_labels)\n",
    "            precision, recall, f1 = micro_f1(true_labels, pred_labels)\n",
    "        auc_score = roc_auc(true_labels, pred_probs, average)\n",
    "    else:\n",
    "        auc_score = -1\n",
    "\n",
    "    output = {\"{}_precision\".format(average): precision, \"{}_recall\".format(average): recall,\n",
    "              \"{}_f1\".format(average): f1, \"{}_accuracy\".format(average): accuracy,\n",
    "              \"{}_auc\".format(average): auc_score, \"{}_P@1\".format(average): p_1, \"{}_P@5\".format(average): p_5,\n",
    "              \"{}_P@8\".format(average): p_8, \"{}_P@10\".format(average): p_10, \"{}_P@15\".format(average): p_15}\n",
    "    \n",
    "    return output\n",
    "\n",
    "def average_scores(scores):\n",
    "    avg_scores = dict()\n",
    "    for key in scores:\n",
    "        if key.startswith(\"level\"):\n",
    "            for metric in scores[key]:\n",
    "                if metric.startswith(\"macro\") or metric.startswith(\"micro\") or metric == \"loss\":\n",
    "                    if metric not in avg_scores:\n",
    "                        avg_scores[metric] = []\n",
    "                    avg_scores[metric].append(scores[key][metric])\n",
    "\n",
    "    for metric in avg_scores:\n",
    "        avg_scores[metric] = sum(avg_scores[metric]) / len(avg_scores[metric])\n",
    "    return avg_scores\n",
    "\n",
    "def union_size(x, y, axis):\n",
    "    return np.logical_or(x, y).sum(axis=axis).astype(float)\n",
    "\n",
    "\n",
    "def intersect_size(x, y, axis):\n",
    "    return np.logical_and(x, y).sum(axis=axis).astype(float)\n",
    "\n",
    "def macro_precision(true_labels, pred_labels):\n",
    "    num = intersect_size(true_labels, pred_labels, 0) / (pred_labels.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "\n",
    "def macro_recall(true_labels, pred_labels):\n",
    "    num = intersect_size(true_labels, pred_labels, 0) / (true_labels.sum(axis=0) + 1e-10)\n",
    "#     print('macro recall num:- ', num)\n",
    "    return np.mean(num)\n",
    "\n",
    "\n",
    "def macro_f1(true_labels, pred_labels):\n",
    "    prec = macro_precision(true_labels, pred_labels)\n",
    "    rec = macro_recall(true_labels, pred_labels)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "\n",
    "def macro_accuracy(true_labels, pred_labels):\n",
    "    num = intersect_size(true_labels, pred_labels, 0) / (union_size(true_labels, pred_labels, 0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "\n",
    "def micro_precision(true_labels, pred_labels):\n",
    "    flat_true = true_labels.ravel()\n",
    "    flat_pred = pred_labels.ravel()\n",
    "    if flat_pred.sum(axis=0) == 0:\n",
    "        return 0.0\n",
    "    return intersect_size(flat_true, flat_pred, 0) / flat_pred.sum(axis=0)\n",
    "\n",
    "\n",
    "def micro_recall(true_labels, pred_labels):\n",
    "#     print('labels shape:- ',true_labels.shape)\n",
    "    flat_true = true_labels.ravel()\n",
    "    flat_pred = pred_labels.ravel()\n",
    "#     print('flattened shape:- ', flat_true.shape)\n",
    "    return intersect_size(flat_true, flat_pred, 0) / flat_true.sum(axis=0)\n",
    "\n",
    "def micro_f1(true_labels, pred_labels):\n",
    "    prec = micro_precision(true_labels, pred_labels)\n",
    "    rec = micro_recall(true_labels, pred_labels)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "\n",
    "def micro_accuracy(true_labels, pred_labels):\n",
    "    flat_true = true_labels.ravel()\n",
    "    flat_pred = pred_labels.ravel()\n",
    "    return intersect_size(flat_true, flat_pred, 0) / union_size(flat_true, flat_pred, 0)\n",
    "\n",
    "\n",
    "def recall_at_k(true_labels, pred_probs, k):\n",
    "    # num true labels in top k predictions / num true labels\n",
    "    sortd = np.argsort(pred_probs)[:, ::-1]\n",
    "    topk = sortd[:, :k]\n",
    "\n",
    "    # get recall at k for each example\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        num_true_in_top_k = true_labels[i, tk].sum()\n",
    "        denom = true_labels[i, :].sum()\n",
    "        vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "    vals = np.array(vals)\n",
    "    vals[np.isnan(vals)] = 0.\n",
    "\n",
    "    return np.mean(vals)\n",
    "\n",
    "\n",
    "def precision_at_k(true_labels, pred_probs, ks=[1, 5, 8, 10, 15]):\n",
    "    # num true labels in top k predictions / k\n",
    "    sorted_pred = np.argsort(pred_probs)[:, ::-1]\n",
    "    output = []\n",
    "    for k in ks:\n",
    "        topk = sorted_pred[:, :k]\n",
    "\n",
    "        # get precision at k for each example\n",
    "        vals = []\n",
    "        for i, tk in enumerate(topk):\n",
    "            if len(tk) > 0:\n",
    "                num_true_in_top_k = true_labels[i, tk].sum()\n",
    "                denom = len(tk)\n",
    "                vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "        output.append(np.mean(vals))\n",
    "    return output\n",
    "\n",
    "\n",
    "def roc_auc(true_labels, pred_probs, average=\"macro\"):\n",
    "    if pred_probs.shape[0] <= 1:\n",
    "        return\n",
    "\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    if average == \"macro\":\n",
    "        # get AUC for each label individually\n",
    "        relevant_labels = []\n",
    "        auc_labels = {}\n",
    "        for i in range(true_labels.shape[1]):\n",
    "            # only if there are true positives for this label\n",
    "            if true_labels[:, i].sum() > 0:\n",
    "                fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], pred_probs[:, i])\n",
    "                if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
    "                    auc_score = auc(fpr[i], tpr[i])\n",
    "                    if not np.isnan(auc_score):\n",
    "                        auc_labels[\"auc_%d\" % i] = auc_score\n",
    "                        relevant_labels.append(i)\n",
    "\n",
    "        # macro-AUC: just average the auc scores\n",
    "        aucs = []\n",
    "        for i in relevant_labels:\n",
    "            aucs.append(auc_labels['auc_%d' % i])\n",
    "        score = np.mean(aucs)\n",
    "    else:\n",
    "        # micro-AUC: just look at each individual prediction\n",
    "        flat_pred = pred_probs.ravel()\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true_labels.ravel(), flat_pred)\n",
    "        score = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def multiclass_roc_auc(true_labels, pred_labels, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(true_labels)\n",
    "    true_labels = lb.transform(true_labels)\n",
    "    pred_labels = lb.transform(pred_labels)\n",
    "    return roc_auc_score(true_labels, pred_labels, average=average)\n",
    "\n",
    "def average_scores(scores):\n",
    "    avg_scores = dict()\n",
    "    for key in scores:\n",
    "        if key.startswith(\"level\"):\n",
    "            for metric in scores[key]:\n",
    "                if metric.startswith(\"macro\") or metric.startswith(\"micro\") or metric == \"loss\":\n",
    "                    if metric not in avg_scores:\n",
    "                        avg_scores[metric] = []\n",
    "                    avg_scores[metric].append(scores[key][metric])\n",
    "\n",
    "    for metric in avg_scores:\n",
    "        avg_scores[metric] = sum(avg_scores[metric]) / len(avg_scores[metric])\n",
    "    return avg_scores\n",
    "\n",
    "def normalise_labels(labels, n_label):\n",
    "    norm_labels = []\n",
    "    for label in labels:\n",
    "        one_hot_vector_label = [0] * n_label\n",
    "        one_hot_vector_label[label] = 1\n",
    "        norm_labels.append(one_hot_vector_label)\n",
    "    return np.asarray(norm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3e2041d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 train_dataloader: TextDataLoader,\n",
    "                 valid_dataloader: TextDataLoader,\n",
    "                 test_dataloader: TextDataLoader,\n",
    "                 criterions,\n",
    "                 optimiser,\n",
    "                 lr_scheduler,\n",
    "                 vocab,\n",
    "                 checkpoint_path,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.criterions = criterions\n",
    "        self.optimiser = optimiser\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.multilabel = multilabel\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.n_training_labels = n_labels\n",
    "\n",
    "        self.main_metric = main_metric\n",
    "\n",
    "        self.start_epoch = 0\n",
    "\n",
    "    def train_single_epoch(self, index):\n",
    "        self.model.train()\n",
    "        \n",
    "        self.train_dataloader.dataset.shuffle_data()\n",
    "        losses = []\n",
    "        true_labels = []\n",
    "        pred_probs = []\n",
    "        ids = []\n",
    "        all_loss_list = []\n",
    "        progress_bar = tqdm.tqdm(self.train_dataloader, unit=\"batches\", desc=\"Training at epoch #{}\".format(index))\n",
    "        progress_bar.clear()\n",
    "        self.optimiser.zero_grad()\n",
    "        batch_id = 0\n",
    "\n",
    "        for text_batch, label_batch, length_batch, id_batch in progress_bar:\n",
    "            batch_id += 1\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            length_batch = length_batch.to(device)\n",
    "\n",
    "            output, attn_weights = self.model(text_batch, length_batch)\n",
    "#             print('output:- ',output.shape)\n",
    "#             print('attn weights shape:- ',attn_weights.shape)\n",
    "\n",
    "            true_labels.extend(label_batch.cpu().numpy())\n",
    "\n",
    "            ids.extend(id_batch)\n",
    "#             output = torch.sigmoid(output)\n",
    "#             print('batch prediction output:-', output)\n",
    "            loss = self.criterions(output, label_batch)\n",
    "#             print(loss)\n",
    "            all_loss_list  = [loss.item()]\n",
    "            output = output.detach().cpu().numpy()\n",
    "            pred_probs.extend(output)\n",
    "            #             print('get_loss output:- ', loss)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.optimiser.step()\n",
    "            self.lr_scheduler.step()\n",
    "            self.optimiser.zero_grad()\n",
    "        \n",
    "#         print('true labels:- ',true_labels)\n",
    "#         print('pred labels', pred_probs[0:10])\n",
    "\n",
    "        scores = OrderedDict()\n",
    "        scores = calculate_eval_metrics(ids, true_labels, pred_probs, self.multilabel)\n",
    "        scores[\"loss\"] = np.mean(all_loss_list).item()\n",
    "\n",
    "        scores[\"average\"] = average_scores(scores)\n",
    "        scores[\"average\"][\"loss\"] = np.mean(losses).item()\n",
    "        progress_bar.refresh(True)\n",
    "        progress_bar.clear(True)\n",
    "        progress_bar.close()\n",
    "#         keys = ['micro_auc', 'macro_auc', 'loss']\n",
    "        print('lr {}, micro_auc {}, macro_auc {}, loss {}'.format(self.lr_scheduler.optimizer.param_groups[0]['lr'], scores['micro_auc'], scores['macro_auc'], scores['loss']))\n",
    "#         print({k:v for k,v in scores.items() if k in keys})\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def format_number(number):\n",
    "        return abs(round(number, ndigits=ndigits))\n",
    "\n",
    "    def train(self, n_epoch: int = 100):\n",
    "        evaluator = Evaluator(self.model, self.vocab, self.criterions, self.n_training_labels)\n",
    "        for e in range(self.start_epoch + 1, n_epoch + 1):\n",
    "            train_scores = self.train_single_epoch(e)\n",
    "            epoch_loss = train_scores[\"average\"][\"loss\"]\n",
    "            valid_scores = evaluator.evaluate(self.valid_dataloader)\n",
    "#             test_scores = evaluator.evaluate(self.test_dataloader)\n",
    "            keys = ['micro_auc', 'macro_auc', 'loss']\n",
    "            print({k:v for k,v in valid_scores.items() if k in keys})\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ee0e163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model(train_data, valid_data, test_data, vocab, saved_data_file_path=None,checkpoint_path=None):\n",
    "    model = RNN(vocab)\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset = _load_and_cache_data(train_data, valid_data, test_data,\n",
    "                                                                      vocab, saved_data_file_path)\n",
    "    train_dataloader = TextDataLoader(dataset=train_dataset, vocab=vocab, batch_size=batch_size)\n",
    "\n",
    "    valid_dataloader = TextDataLoader(dataset=valid_dataset, vocab=vocab, batch_size=batch_size)\n",
    "\n",
    "    test_dataloader = TextDataLoader(dataset=test_dataset, vocab=vocab, batch_size=batch_size)\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters())\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size = 30)\n",
    "    \n",
    "    criterions = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      valid_dataloader=valid_dataloader,\n",
    "                      test_dataloader=test_dataloader,\n",
    "                      criterions=criterions,\n",
    "                      optimiser=optimiser,\n",
    "                      lr_scheduler = lr_scheduler,\n",
    "                      vocab=vocab,\n",
    "                      checkpoint_path=checkpoint_path)\n",
    "    best_model = trainer.train(n_epoch=n_epoch)\n",
    "\n",
    "    evaluator = Evaluator(model=best_model,vocab=vocab,criterions=criterions,\n",
    "                          n_training_labels=n_labels)\n",
    "\n",
    "    del model, optimiser, lr_scheduler, evaluator, trainer, criterions\n",
    "    return best_model, scores  # either on valid or test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "13164513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_validation(training_data, valid_data, test_data, vocab, saved_data_file_path):\n",
    "    best_model, scores = _train_model(\n",
    "        train_data=training_data, valid_data=valid_data, test_data=test_data,\n",
    "        vocab=vocab, saved_data_file_path=saved_data_file_path, checkpoint_path=\"../model/checkpoint.pkl\")\n",
    "\n",
    "    return best_model, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "772dacdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853a1cd54f7b4b779c322ec12333fc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training at epoch #1:   0%|          | 0/1009 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.000000000000002e-36, micro_auc 0.6565756279721523, macro_auc 0.5046072789197418, loss 0.4284425973892212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cdda06ae14468a91ed26dd361b2e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/197 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macro_auc': 0.5048241006345205, 'micro_auc': 0.6452702552802831, 'loss': 0.2531854808330536}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9eb9af1c6d41e7986bb5c144d011fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training at epoch #2:   0%|          | 0/1009 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0000000000000042e-70, micro_auc 0.6616766133116669, macro_auc 0.5063644772817476, loss 0.40936529636383057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8680447a5b564fd38483c39edf15b43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/197 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macro_auc': 0.5048241006345205, 'micro_auc': 0.6452702552802831, 'loss': 0.2531854808330536}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09beb16080445c9996523661784587a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training at epoch #3:   0%|          | 0/1009 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0000000000000062e-103, micro_auc 0.661653861767094, macro_auc 0.5061893725668211, loss 0.3711094260215759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85aa30c4f96413b9fc93152d69f89fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/197 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macro_auc': 0.5048241006345205, 'micro_auc': 0.6452702552802831, 'loss': 0.2531854808330536}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77071117efa4915b4a3b3ef47f669d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training at epoch #4:   0%|          | 0/1009 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [268], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_data_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.data.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_vocab_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [267], line 2\u001b[0m, in \u001b[0;36mrun_with_validation\u001b[0;34m(training_data, valid_data, test_data, vocab, saved_data_file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_validation\u001b[39m(training_data, valid_data, test_data, vocab, saved_data_file_path):\n\u001b[0;32m----> 2\u001b[0m     best_model, scores \u001b[38;5;241m=\u001b[39m \u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_data_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaved_data_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../model/checkpoint.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model, scores\n",
      "Cell \u001b[0;32mIn [266], line 28\u001b[0m, in \u001b[0;36m_train_model\u001b[0;34m(train_data, valid_data, test_data, vocab, saved_data_file_path, checkpoint_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m criterions \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     20\u001b[0m                   train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     21\u001b[0m                   valid_dataloader\u001b[38;5;241m=\u001b[39mvalid_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m                   vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     27\u001b[0m                   checkpoint_path\u001b[38;5;241m=\u001b[39mcheckpoint_path)\n\u001b[0;32m---> 28\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(model\u001b[38;5;241m=\u001b[39mbest_model,vocab\u001b[38;5;241m=\u001b[39mvocab,criterions\u001b[38;5;241m=\u001b[39mcriterions,\n\u001b[1;32m     31\u001b[0m                       n_training_labels\u001b[38;5;241m=\u001b[39mn_labels)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model, optimiser, lr_scheduler, evaluator, trainer, criterions\n",
      "Cell \u001b[0;32mIn [265], line 95\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epoch)\u001b[0m\n\u001b[1;32m     93\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_training_labels)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     train_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m train_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     97\u001b[0m     valid_scores \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dataloader)\n",
      "Cell \u001b[0;32mIn [265], line 49\u001b[0m, in \u001b[0;36mTrainer.train_single_epoch\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     46\u001b[0m             label_batch \u001b[38;5;241m=\u001b[39m label_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m             length_batch \u001b[38;5;241m=\u001b[39m length_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 49\u001b[0m             output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#             print('output:- ',output.shape)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#             print('attn weights shape:- ',attn_weights.shape)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m             true_labels\u001b[38;5;241m.\u001b[39mextend(label_batch\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [257], line 52\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, batch_data, lengths)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mflatten_parameters()\n\u001b[1;32m     50\u001b[0m embeds \u001b[38;5;241m=\u001b[39m pack_padded_sequence(embeds, lengths\u001b[38;5;241m.\u001b[39mcpu(), batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 52\u001b[0m rnn_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_model\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     54\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 772\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    774\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    775\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_with_validation(training_data, valid_data, test_data, vocab, saved_data_file_path=\"{}.data.pkl\".format(saved_vocab_path.split(\".pkl\")[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
